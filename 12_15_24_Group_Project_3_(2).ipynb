{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aw3444/blank-app/blob/main/12_15_24_Group_Project_3_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685833a2",
      "metadata": {
        "id": "685833a2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbfb0a7",
      "metadata": {
        "id": "cfbfb0a7"
      },
      "source": [
        "# Part 1 - Reading and Preprocessing Data\n",
        "In this part, we define the load_data and split_data functions. They are in charge of loading the data from the csv files and splitting them into training and testing splits respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181478d9",
      "metadata": {
        "id": "181478d9"
      },
      "source": [
        "Load Data Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b4ee1bc",
      "metadata": {
        "id": "8b4ee1bc"
      },
      "outputs": [],
      "source": [
        "#define load_data function\n",
        "def load_data(filename):\n",
        "    #load csv as dataframe, then convert to np array\n",
        "    df = pd.read_csv(filename)\n",
        "    data = df.to_numpy()\n",
        "\n",
        "    #the data is a numpy array, but since the values were separated by semi-colons,\n",
        "    #the nd array has only 1 column. In the next part reformat this array\n",
        "    #so that it has 11 columns:\n",
        "\n",
        "\n",
        "    #create an array that matches the dimensionality of our data\n",
        "    formatted_data = np.zeros([len(data),11])\n",
        "    #print(formatted_data.shape)\n",
        "    i = 0\n",
        "\n",
        "    #iterate through each row of the data, create a new array by splitting the ; array\n",
        "    #insert the new array as a row in formatted_data\n",
        "    for row in data:\n",
        "        formatted_row = str(row[0]).split(';')\n",
        "        formatted_row = np.array(formatted_row)\n",
        "        formatted_data[i] = formatted_row[0:11]\n",
        "        i += 1\n",
        "\n",
        "    #print(formatted_data)\n",
        "\n",
        "\n",
        "    #return the formatted array!\n",
        "    return formatted_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc520bc9",
      "metadata": {
        "id": "fc520bc9"
      },
      "source": [
        "Split Data Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6726d051",
      "metadata": {
        "id": "6726d051"
      },
      "outputs": [],
      "source": [
        "#define split_data function\n",
        "def split_data(dataset, ratio):\n",
        "\n",
        "    #get the length of the dataset\n",
        "    length = len(dataset)\n",
        "\n",
        "    #the length of the training set\n",
        "    train_length = int(length * ratio)\n",
        "\n",
        "    #isolate the training set\n",
        "    train = dataset[0:train_length]\n",
        "\n",
        "    #isolate the test set\n",
        "    test = dataset[train_length:]\n",
        "\n",
        "    #return the traiing and test sets as a tuple\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07d90517",
      "metadata": {
        "id": "07d90517"
      },
      "source": [
        "Next, we use these functions to check if they work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bee55256",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bee55256",
        "outputId": "a8b94f43-9887-4da6-e686-fbf6ebe26e3c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'redwine.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-17c00cad1a59>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#first get the nd array of the data from the csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mredwine_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'redwine.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Then get a train/test tuple with the split data function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mredwine_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredwine_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-62e135a5abfa>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#load csv as dataframe, then convert to np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'redwine.csv'"
          ]
        }
      ],
      "source": [
        "#first get the nd array of the data from the csv\n",
        "redwine_data = load_data('redwine.csv')\n",
        "\n",
        "#Then get a train/test tuple with the split data function\n",
        "redwine_dataset = split_data(redwine_data,0.9)\n",
        "\n",
        "#Check the data\n",
        "redwine_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fec553",
      "metadata": {
        "id": "d7fec553"
      },
      "source": [
        "The data looks properly formatted! Next we load in the data for the white wine dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8deaade3",
      "metadata": {
        "id": "8deaade3"
      },
      "outputs": [],
      "source": [
        "whitewine_data = load_data('whitewine.csv')\n",
        "\n",
        "whitewine_dataset = split_data(whitewine_data,0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9cecd0",
      "metadata": {
        "id": "2f9cecd0"
      },
      "source": [
        "Then, we separate each dataset into its respective training and test sets, and print out the shapes to make sure that everything is working properly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08419180",
      "metadata": {
        "id": "08419180"
      },
      "outputs": [],
      "source": [
        "rw_train, rw_test = redwine_dataset[0], redwine_dataset[1]\n",
        "ww_train, ww_test = whitewine_dataset[0], whitewine_dataset[1]\n",
        "\n",
        "print(f\"Red Wine Training Data Shape: {rw_train.shape}\")\n",
        "print(f\"Red Wine Testing Data Shape: {rw_test.shape}\")\n",
        "print()\n",
        "\n",
        "print(f\"White Wine Training Data Shape: {ww_train.shape}\")\n",
        "print(f\"White Wine Testing Data Shape: {ww_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74fd70aa",
      "metadata": {
        "id": "74fd70aa"
      },
      "source": [
        "The shapes look good! Time to move on to part 2!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d089f72e",
      "metadata": {
        "id": "d089f72e"
      },
      "source": [
        "# Part 2 - Nearest Centroid Classifier\n",
        "In this part, we define all the necessary functions to perform Nearest Centroid Classification on the data. The required functions are the Compute Centroid Function, the Get Distance Function, and the Experiment function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a379431",
      "metadata": {
        "id": "7a379431"
      },
      "source": [
        "Compute Centroid Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543e7714",
      "metadata": {
        "id": "543e7714"
      },
      "outputs": [],
      "source": [
        "#Modeled off the lecture notes\n",
        "def compute_centroid(samples):\n",
        "    #sum the data from all of the samples and get the average point\n",
        "    return sum(samples[:,:]) / samples.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58eb5ab1",
      "metadata": {
        "id": "58eb5ab1"
      },
      "source": [
        "Get Distance Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9469821a",
      "metadata": {
        "id": "9469821a"
      },
      "outputs": [],
      "source": [
        "#Again modeled off of the lecture notes\n",
        "def get_distance(data,centroid):\n",
        "    #calculate euclidean distance between a datapoint and the centroid\n",
        "    return np.linalg.norm(data-centroid)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de990cfe",
      "metadata": {
        "id": "de990cfe"
      },
      "source": [
        "Experiment Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf403219",
      "metadata": {
        "id": "bf403219"
      },
      "outputs": [],
      "source": [
        "def experiment(ww_training, ww_test, rw_training, rw_test):\n",
        "\n",
        "    #First determine the centroids for each class using the compute_centroid fucntion\n",
        "    ww_centroid = compute_centroid(ww_training)\n",
        "    rw_centroid = compute_centroid(rw_training)\n",
        "    #It is important to note that the centroids are ONLY calculated using the training set\n",
        "\n",
        "\n",
        "\n",
        "    #Then declare two variables, one keeps track of the number of correct predictions\n",
        "    #The other keeps track of the total number of predictions\n",
        "    correct = 0\n",
        "    predictions = 0\n",
        "\n",
        "    #Iterate through the rows in the test set for the white wine\n",
        "    for row in ww_test:\n",
        "\n",
        "        #Increment predictions\n",
        "        predictions += 1\n",
        "\n",
        "        #If the point is closer to the white wine centroid, then the classifier is correct!\n",
        "        if get_distance(row,ww_centroid) < get_distance(row,rw_centroid):\n",
        "            label = \"white\"\n",
        "            correct += 1\n",
        "        else:\n",
        "            #if not then the classifier is wrong\n",
        "            label = \"red\"\n",
        "\n",
        "    #Do the same iterative process on the red wine data\n",
        "    for row in rw_test:\n",
        "        predictions += 1\n",
        "        if get_distance(row,ww_centroid) < get_distance(row,rw_centroid):\n",
        "            label = \"white\"\n",
        "        else:\n",
        "            label = \"red\"\n",
        "            correct += 1\n",
        "\n",
        "    #Determine accuracy by normalizing correct prediction count by total number of predictions\n",
        "    accuracy = correct / predictions\n",
        "\n",
        "    #Print out required stats\n",
        "    print(f\"Total Predictions: {predictions}\\nTotal Correct: {correct}\\nAccuracy: {accuracy}\")\n",
        "\n",
        "    #Return the accuracy\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d08bb899",
      "metadata": {
        "id": "d08bb899"
      },
      "source": [
        "With the model defined, we can run the experiment to see how well it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d52d49",
      "metadata": {
        "scrolled": false,
        "id": "51d52d49"
      },
      "outputs": [],
      "source": [
        "model_accuracy = experiment(ww_train,ww_test,rw_train,rw_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b51ca029",
      "metadata": {
        "id": "b51ca029"
      },
      "source": [
        "The nearest centroid model appears to do quite well, boasting almost a 91% accuracy rate. In the next step, we perform cross validation to ensure that this high accuracy rate is not just a fluke in the test set:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0cd1c3",
      "metadata": {
        "id": "dd0cd1c3"
      },
      "source": [
        "# Part 3 - Cross Validation\n",
        "In this step, we perform cross validation. To do this, we define the Cross Validation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e637f0c",
      "metadata": {
        "id": "0e637f0c"
      },
      "outputs": [],
      "source": [
        "def cross_validation(ww_data, rw_data,k):\n",
        "    #Set the average accuracy intially to 0\n",
        "    average_accuracy = 0\n",
        "\n",
        "    #Define a step size that is a function of k\n",
        "    step_size = int(len(ww_data)/k)\n",
        "    #print(step_size)\n",
        "\n",
        "    #Iterate k times through the dataset, using the classifying model each repetition\n",
        "    for i in range(k):\n",
        "        print(f\"Experiment {i+1}:\")\n",
        "\n",
        "        #First split up the dataset - begin by getting the starting index of the test set\n",
        "        test_start = i*step_size\n",
        "\n",
        "        #If, for instance, i is the last iteration in the loop, we might cut off some\n",
        "        #Data at the end, especially if the step size is not a whole number\n",
        "        #The following if statement accounts for this\n",
        "        #This means that the test set for the last loop will be slightly larger\n",
        "        if i != k-1:\n",
        "            test_end = test_start + step_size\n",
        "        else:\n",
        "            test_end = len(ww_data)-1\n",
        "\n",
        "        #print(f\"\\nTest Start Index: {test_start}\\nTest End Index: {test_end}\\n\")\n",
        "\n",
        "        #Slice the dataset with the starting and ending indices of the test set\n",
        "        ww_test = ww_data[test_start:test_end]\n",
        "        rw_test = rw_data[test_start:test_end]\n",
        "\n",
        "        #Then stack the remainder of the data to create the training set\n",
        "        ww_train = np.vstack((ww_data[0:test_start],ww_data[test_end:]))\n",
        "        rw_train = np.vstack((rw_data[0:test_start],rw_data[test_end:]))\n",
        "\n",
        "        #With the data formatted, run the model\n",
        "        model_accuracy = experiment(ww_train,ww_test,rw_train,rw_test)\n",
        "\n",
        "        #Increment the average accuracy\n",
        "        average_accuracy += model_accuracy\n",
        "\n",
        "        #Print statement for formatting purposes\n",
        "        print()\n",
        "\n",
        "    #Finally, divide the average accuracy by k to make it a real average and not just a sum\n",
        "    average_accuracy /= k\n",
        "\n",
        "    #Return the average accuracy\n",
        "    return average_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ea81e3",
      "metadata": {
        "id": "d1ea81e3"
      },
      "source": [
        "With the cross validation function complete, we can see exactly how well the model works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd9332e",
      "metadata": {
        "scrolled": false,
        "id": "8bd9332e"
      },
      "outputs": [],
      "source": [
        "#Using k = 20 to ensure robust results and sufficiently sized training data\n",
        "avg_accuracy = cross_validation(whitewine_data, redwine_data,20)\n",
        "print(f\"Average Accuracy: {avg_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29455326",
      "metadata": {
        "id": "29455326"
      },
      "source": [
        "The average accuracy is right around 88%. This is an indicator that the model performs well!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e96d856f",
      "metadata": {
        "id": "e96d856f"
      },
      "source": [
        "Thanks for a great semester!\n",
        "\n",
        "-Ming, Joyce, and Robert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca31e3f2",
      "metadata": {
        "id": "ca31e3f2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}